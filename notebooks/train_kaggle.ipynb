{
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.12"
        },
        "kaggle": {
            "accelerator": "nvidiaTeslaT4",
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook",
            "isGpuEnabled": true
        }
    },
    "nbformat_minor": 4,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": "# ðŸš€ DeepSeek LeetCode Fine-Tuning (Kaggle GPU)\n\n**Settings:** Accelerator â†’ GPU (P100/T4)\n\nThis notebook fine-tunes DeepSeek-Coder-6.7B on LeetCode problems using LoRA.\n\n**Key Settings:**\n- `dataset_text_field=\"text\"` - Required for SFTTrainer\n- `max_seq_length=2048` - Truncate long examples\n- `packing=False` - Disable sequence packing",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Install dependencies\n!pip install -q transformers datasets accelerate peft bitsandbytes\n!pip install -q trl==0.12.0",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Verify GPU\nimport torch\nprint(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'NO GPU!'}\")\nassert torch.cuda.is_available(), \"GPU not available! Enable in Settings â†’ Accelerator â†’ GPU\"",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Load Dataset\nfrom datasets import load_dataset, Dataset\nds = load_dataset(\"LongQ/leetcode_python\", split=\"train\")\nprint(f\"âœ… Loaded {len(ds)} problems\")\nprint(f\"Fields: {ds.column_names}\")",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Format Training Examples\ndef format_example(ex):\n    description = ex.get('problem_description', '')[:2000]\n    starter = ex.get('starter_code', '')\n    solution = ex.get('solution_code', '')\n    \n    if not solution or not description:\n        return None\n    \n    text = f\"\"\"### Problem:\\n{description}\\n\\n### Starter Code:\\n```python\\n{starter}\\n```\\n\\n### Solution:\\n```python\\n{solution}\\n```\"\"\"\n    return {\"text\": text}\n\ndata = [result for ex in ds if (result := format_example(ex)) is not None]\ntrain_ds = Dataset.from_list(data)\nprint(f\"âœ… Training examples: {len(train_ds)}\")\nprint(f\"Sample:\\n{train_ds[0]['text'][:500]}\")",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Load Model with 4-bit Quantization\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nMODEL = \"deepseek-ai/deepseek-coder-6.7b-base\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\nprint(f\"âœ… Model loaded: {MODEL}\")",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Configure LoRA\nmodel = prepare_model_for_kbit_training(model)\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, lora_config)\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in model.parameters())\nprint(f\"âœ… LoRA: {trainable:,} trainable / {total:,} total ({100*trainable/total:.2f}%)\")",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Training Configuration\nfrom trl import SFTTrainer, SFTConfig\n\ntraining_config = SFTConfig(\n    output_dir=\"/kaggle/working/deepseek-leetcode\",\n    num_train_epochs=3,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    learning_rate=2e-4,\n    warmup_ratio=0.03,\n    weight_decay=0.01,\n    logging_steps=25,\n    save_strategy=\"steps\",\n    save_steps=100,\n    save_total_limit=2,\n    fp16=True,\n    optim=\"paged_adamw_8bit\",\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    \n    # Critical settings\n    dataset_text_field=\"text\",\n    max_seq_length=2048,\n    packing=False,\n    \n    report_to=\"none\",\n    seed=42,\n)\n\nprint(\"âœ… Training config ready\")",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Train!\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_ds,\n    args=training_config,\n    tokenizer=tokenizer,\n)\n\nprint(\"ðŸš€ Starting training...\")\ntrainer.train()\nprint(\"âœ… Training complete!\")",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Test Generation\ntest_prompt = \"\"\"### Problem:\nGiven an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n\n### Starter Code:\n```python\nclass Solution:\n    def twoSum(self, nums: List[int], target: int) -> List[int]:\n```\n\n### Solution:\n```python\n\"\"\"\n\ninputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\nmodel.eval()\n\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=200,\n        temperature=0.2,\n        do_sample=True,\n        top_p=0.95,\n        pad_token_id=tokenizer.eos_token_id,\n    )\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## ðŸ“¤ Upload to HuggingFace\n\nMerge LoRA weights into base model and upload as FP16.",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Login to HuggingFace\n!pip install -q huggingface_hub\nfrom huggingface_hub import login\nlogin()",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": "# Merge LoRA and Upload as FP16\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\nMODEL = \"deepseek-ai/deepseek-coder-6.7b-base\"\nADAPTER = \"/kaggle/working/deepseek-leetcode\"  # Use latest checkpoint\n\n# Load base model in FP16 on CPU (avoids GPU OOM)\nprint(\"Loading base model (FP16)...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    MODEL, \n    torch_dtype=torch.float16,\n    device_map=\"cpu\",\n    trust_remote_code=True,\n    low_cpu_mem_usage=True\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n\nprint(\"Loading adapter...\")\nmodel = PeftModel.from_pretrained(base_model, ADAPTER)\n\nprint(\"Merging...\")\nmodel = model.merge_and_unload()\n\nprint(\"Saving locally...\")\nmodel.save_pretrained(\"/kaggle/working/merged\", safe_serialization=True)\ntokenizer.save_pretrained(\"/kaggle/working/merged\")\n\nprint(\"Uploading to HuggingFace...\")\nmodel.push_to_hub(\"YOUR_USERNAME/deepseek-leetcode-fp16\")  # Change this!\ntokenizer.push_to_hub(\"YOUR_USERNAME/deepseek-leetcode-fp16\")\n\nprint(\"âœ… Done!\")",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## ðŸ”§ Convert to GGUF (for Ollama)\n\nRun this in a **fresh Kaggle session** to avoid OOM issues.",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Download from HuggingFace and convert to GGUF\n!pip install -q transformers huggingface_hub\n!git clone https://github.com/ggerganov/llama.cpp\n!pip install -q -r llama.cpp/requirements.txt\n\nfrom huggingface_hub import snapshot_download\nsnapshot_download(\n    \"YOUR_USERNAME/deepseek-leetcode-fp16\",  # Change this!\n    local_dir=\"/kaggle/working/merged\",\n)\nprint(\"âœ… Downloaded!\")\n\n# Convert to GGUF Q8\n!python llama.cpp/convert_hf_to_gguf.py /kaggle/working/merged \\\n    --outfile /kaggle/working/deepseek-leetcode-q8.gguf \\\n    --outtype q8_0\n\nprint(\"âœ… Converted! Download deepseek-leetcode-q8.gguf from Output tab\")",
            "metadata": {},
            "outputs": [],
            "execution_count": null
        }
    ]
}