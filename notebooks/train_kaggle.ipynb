{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14220466,"sourceType":"datasetVersion","datasetId":9071233},{"sourceId":14239499,"sourceType":"datasetVersion","datasetId":9084761},{"sourceId":693669,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":526028,"modelId":540086}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ğŸš€ DeepSeek LeetCode Fine-Tuning (KAGGLE GPU - FIXED)\n\n**Settings:** Accelerator â†’ GPU (P100/T4)\n\n**Key Fixes:**\n- Added `dataset_text_field=\"text\"` (critical!)\n- Added `max_seq_length=2048`\n- Fixed TRL installation order\n- Added `packing=False`","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install dependencies FIRST (before imports)\n!pip install -q transformers datasets accelerate peft bitsandbytes\n!pip install -q trl==0.12.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T19:43:29.715327Z","iopub.execute_input":"2025-12-21T19:43:29.715530Z","iopub.status.idle":"2025-12-21T19:43:40.366867Z","shell.execute_reply.started":"2025-12-21T19:43:29.715509Z","shell.execute_reply":"2025-12-21T19:43:40.366013Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Verify GPU\nimport torch\nprint(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'NO GPU!'}\")\nassert torch.cuda.is_available(), \"GPU not available! Enable in Settings â†’ Accelerator â†’ GPU\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T19:44:50.884617Z","iopub.execute_input":"2025-12-21T19:44:50.884963Z","iopub.status.idle":"2025-12-21T19:44:54.663116Z","shell.execute_reply.started":"2025-12-21T19:44:50.884897Z","shell.execute_reply":"2025-12-21T19:44:54.662351Z"}},"outputs":[{"name":"stdout","text":"GPU: Tesla T4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset, Dataset\nds = load_dataset(\"LongQ/leetcode_python\", split=\"train\")  # â† CHANGE 1\nprint(f\"âœ… Loaded {len(ds)} problems\")\nprint(f\"Fields: {ds.column_names}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T19:44:54.664426Z","iopub.execute_input":"2025-12-21T19:44:54.664845Z","iopub.status.idle":"2025-12-21T19:45:00.083077Z","shell.execute_reply.started":"2025-12-21T19:44:54.664822Z","shell.execute_reply":"2025-12-21T19:45:00.082461Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/861 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1eeec30049794f7db29378dafe066394"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/13.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a01334bad4c4022b1f4b0fd9980e7de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/dev-00000-of-00001.parquet:   0%|          | 0.00/1.17M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc65bfc418134ddaaf1b6dec86fa6469"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/1.17M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"990a156711084741a613e83cd1f0b034"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2369 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0a8e61995a742e289880710648398d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"677c3039f6074afbb478ebe10d214ed5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5177b7042e44244b27ff9ba54b66411"}},"metadata":{}},{"name":"stdout","text":"âœ… Loaded 2369 problems\nFields: ['id', 'difficulty', 'tags', 'problem_description', 'starter_code', 'solution_code', 'context_code', 'entry_code', 'test_code', 'prompt', 'completion']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def format_example(ex):\n    description = ex.get('problem_description', '')[:2000]\n    starter = ex.get('starter_code', '')\n    solution = ex.get('solution_code', '')  # â† CHANGE 2: 'completion' â†’ 'solution_code'\n    \n    if not solution or not description:\n        return None\n    \n    text = f\"\"\"### Problem:\\n{description}\\n\\n### Starter Code:\\n```python\\n{starter}\\n```\\n\\n### Solution:\\n```python\\n{solution}\\n```\"\"\"\n    return {\"text\": text}\ndata = [result for ex in ds if (result := format_example(ex)) is not None]\ntrain_ds = Dataset.from_list(data)\nprint(f\"âœ… Training examples: {len(train_ds)}\")\nprint(f\"Sample:\\n{train_ds[0]['text'][:500]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T19:45:02.359470Z","iopub.execute_input":"2025-12-21T19:45:02.359756Z","iopub.status.idle":"2025-12-21T19:45:02.612405Z","shell.execute_reply.started":"2025-12-21T19:45:02.359731Z","shell.execute_reply":"2025-12-21T19:45:02.611787Z"}},"outputs":[{"name":"stdout","text":"âœ… Training examples: 2369\nSample:\n### Problem:\nGiven an array of integers numsÂ and an integer target, return indices of the two numbers such that they add up to target.\nYou may assume that each input would have exactly one solution, and you may not use the same element twice.\nYou can return the answer in any order.\nÂ \nExample 1:\n\nInput: nums = [2,7,11,15], target = 9\nOutput: [0,1]\nExplanation: Because nums[0] + nums[1] == 9, we return [0, 1].\n\nExample 2:\n\nInput: nums = [3,2,4], target = 6\nOutput: [1,2]\n\nExample 3:\n\nInput: nums = \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Load Model with 4-bit Quantization\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nMODEL = \"deepseek-ai/deepseek-coder-6.7b-base\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\nprint(f\"âœ… Model loaded: {MODEL}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T19:45:05.288991Z","iopub.execute_input":"2025-12-21T19:45:05.289274Z","iopub.status.idle":"2025-12-21T19:46:32.716989Z","shell.execute_reply.started":"2025-12-21T19:45:05.289250Z","shell.execute_reply":"2025-12-21T19:46:32.716349Z"}},"outputs":[{"name":"stderr","text":"2025-12-21 19:45:14.674703: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766346314.857370      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766346314.914582      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766346315.373435      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766346315.373471      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766346315.373474      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766346315.373477      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/793 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e01a43b63e4d43a6a8d8a0aa29c8f924"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"764beeccae484e928079cf043765f441"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/632 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3a36d5a93e94a4f903975772be37c83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"901c494bb5cd4c7fb2e37313b30354b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80a35b96326d49e69d11c02b8a586146"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2bbcc18ed6b461d96254810fd78be74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c95911a5bf7b4546a723249c62d57eb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d5a8fc6a2b34869b61e416fdc3e6b2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1abb37f82bd24070afdb403f03533d16"}},"metadata":{}},{"name":"stdout","text":"âœ… Model loaded: deepseek-ai/deepseek-coder-6.7b-base\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Configure LoRA\nmodel = prepare_model_for_kbit_training(model)\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, lora_config)\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in model.parameters())\nprint(f\"âœ… LoRA: {trainable:,} trainable / {total:,} total ({100*trainable/total:.2f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T19:46:39.568758Z","iopub.execute_input":"2025-12-21T19:46:39.569155Z","iopub.status.idle":"2025-12-21T19:46:40.165002Z","shell.execute_reply.started":"2025-12-21T19:46:39.569123Z","shell.execute_reply":"2025-12-21T19:46:40.164265Z"}},"outputs":[{"name":"stdout","text":"âœ… LoRA: 39,976,960 trainable / 3,542,487,040 total (1.13%)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Training Config with Epoch-End Zip & Download\nfrom trl import SFTTrainer, SFTConfig\nfrom transformers import TrainerCallback\nimport shutil\nimport os\nfrom IPython.display import FileLink, display\nclass ZipAndDownloadCallback(TrainerCallback):\n    \"\"\"Callback to zip and download model after each epoch.\"\"\"\n    \n    def on_epoch_end(self, args, state, control, **kwargs):\n        epoch = int(state.epoch)\n        output_dir = args.output_dir\n        \n        # Create zip filename\n        zip_name = f\"deepseek-leetcode-epoch{epoch}\"\n        zip_path = f\"/kaggle/working/{zip_name}\"\n        \n        print(f\"\\nğŸ“¦ Zipping model after epoch {epoch}...\")\n        \n        # Zip the output directory\n        shutil.make_archive(zip_path, 'zip', output_dir)\n        \n        print(f\"âœ… Created: {zip_path}.zip\")\n        \n        # Display download link (works in Kaggle notebooks)\n        display(FileLink(f\"{zip_path}.zip\", result_html_prefix=f\"â¬‡ï¸ Download Epoch {epoch}: \"))\n        \n        return control\ntraining_config = SFTConfig(\n    output_dir=\"/kaggle/working/deepseek-leetcode\",\n    num_train_epochs=3,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    learning_rate=2e-4,\n    warmup_ratio=0.03,\n    weight_decay=0.01,\n    logging_steps=25,\n    save_strategy=\"steps\",\n    save_steps=100,\n    save_total_limit=2,\n    fp16=True,\n    optim=\"paged_adamw_8bit\",\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    \n    # CRITICAL FIXES\n    dataset_text_field=\"text\",\n    max_seq_length=2048,\n    packing=False,\n    \n    report_to=\"none\",\n    seed=42,\n)\n# Create the callback instance\nzip_callback = ZipAndDownloadCallback()\nprint(\"âœ… Config ready with epoch-end zip & download\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T19:46:40.166006Z","iopub.execute_input":"2025-12-21T19:46:40.166370Z","iopub.status.idle":"2025-12-21T19:46:44.133736Z","shell.execute_reply.started":"2025-12-21T19:46:40.166344Z","shell.execute_reply":"2025-12-21T19:46:44.132991Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"âœ… Config ready with epoch-end zip & download\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, lora_config)  # â† This wraps the model with LoRA\nprint(f\"âœ… LoRA applied\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T19:47:50.409727Z","iopub.execute_input":"2025-12-21T19:47:50.410440Z","iopub.status.idle":"2025-12-21T19:47:51.484919Z","shell.execute_reply.started":"2025-12-21T19:47:50.410407Z","shell.execute_reply":"2025-12-21T19:47:51.484070Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"âœ… LoRA applied\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train!\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_ds,\n    args=training_config,\n    tokenizer=tokenizer,\n)\n\nprint(\"ğŸš€ Starting training...\")\ntrainer.train()\nprint(\"âœ… Training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T19:47:55.055223Z","iopub.execute_input":"2025-12-21T19:47:55.055595Z","iopub.status.idle":"2025-12-22T01:07:48.098330Z","shell.execute_reply.started":"2025-12-21T19:47:55.055563Z","shell.execute_reply":"2025-12-22T01:07:48.097643Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2369 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4026c1f554f24ca69a266a3cf00d41af"}},"metadata":{}},{"name":"stderr","text":"The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32014}.\n","output_type":"stream"},{"name":"stdout","text":"ğŸš€ Starting training...\n","output_type":"stream"},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='891' max='891' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [891/891 5:19:28, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>0.241300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.141300</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.148100</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.133400</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.135300</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.143200</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.140600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.150600</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.123300</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.154900</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.115800</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.120900</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>0.121700</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.098600</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>0.098900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.099700</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>0.098200</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.095600</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>0.098200</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.122800</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>0.084700</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.111800</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>0.113000</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.106900</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>0.072600</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.072900</td>\n    </tr>\n    <tr>\n      <td>675</td>\n      <td>0.069200</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.081600</td>\n    </tr>\n    <tr>\n      <td>725</td>\n      <td>0.085900</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.074300</td>\n    </tr>\n    <tr>\n      <td>775</td>\n      <td>0.070600</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.070400</td>\n    </tr>\n    <tr>\n      <td>825</td>\n      <td>0.073000</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.075900</td>\n    </tr>\n    <tr>\n      <td>875</td>\n      <td>0.085300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"âœ… Training complete!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Test Generation\ntest_prompt = \"\"\"### Problem:\nGiven an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n\n### Starter Code:\n```python\nclass Solution:\n    def twoSum(self, nums: List[int], target: int) -> List[int]:\n```\n\n### Solution:\n```python\n\"\"\"\n\ninputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\nmodel.eval()\n\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=200,\n        temperature=0.2,\n        do_sample=True,\n        top_p=0.95,\n        pad_token_id=tokenizer.eos_token_id,\n    )\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T01:30:03.609594Z","iopub.execute_input":"2025-12-22T01:30:03.610250Z","iopub.status.idle":"2025-12-22T01:30:13.547878Z","shell.execute_reply.started":"2025-12-22T01:30:03.610219Z","shell.execute_reply":"2025-12-22T01:30:13.546850Z"}},"outputs":[{"name":"stdout","text":"### Problem:\nGiven an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n\n### Starter Code:\n```python\nclass Solution:\n    def twoSum(self, nums: List[int], target: int) -> List[int]:\n```\n\n### Solution:\n```python\nclass Solution:\n    def twoSum(self, nums: List[int], target: int) -> List[int]:\n        for i in range(len(nums)):\n            for j in range(i + 1, len(nums)):\n                if nums[i] + nums[j] == target:\n                    return [i, j]\n```\n\n### Explanation:\nThe solution iterates through the list of numbers and checks if the sum of any two numbers is equal to the target. If it is, it returns the indices of those numbers.\n\n### Complexity:\n- Time complexity: O(n^2)\n- Space complexity: O(1)\n\n### Test Cases:\n```python\nnums = [2, 7, 11, 15]\ntarget = 9\nprint(Solution().twoSum(nums\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"!pip install -q transformers peft accelerate\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nMODEL = \"deepseek-ai/deepseek-coder-6.7b-base\"\nADAPTER = \"/kaggle/working/deepseek-leetcode/checkpoint-891\"\n# NO bitsandbytes - load in FP16 on CPU\nprint(\"Loading base model (FP16)...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL, \n    torch_dtype=torch.float16,\n    device_map=\"cpu\",  # CPU to avoid GPU OOM\n    trust_remote_code=True,\n    low_cpu_mem_usage=True\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\nprint(\"Loading adapter...\")\nmodel = PeftModel.from_pretrained(model, ADAPTER)\nprint(\"Merging...\")\nmodel = model.merge_and_unload()\nprint(\"Saving as FP16...\")\nmodel.save_pretrained(\"/kaggle/working/merged\", safe_serialization=True)\ntokenizer.save_pretrained(\"/kaggle/working/merged\")\nprint(\"Uploading to HuggingFace...\")\nmodel.push_to_hub(\"Jerry-lin23/deepseek-leetcode-fp16\")\ntokenizer.push_to_hub(\"Jerry-lin23/deepseek-leetcode-fp16\")\nprint(\"âœ… Done!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T01:53:35.016702Z","iopub.execute_input":"2025-12-22T01:53:35.017141Z","iopub.status.idle":"2025-12-22T01:59:38.950921Z","shell.execute_reply.started":"2025-12-22T01:53:35.017113Z","shell.execute_reply":"2025-12-22T01:59:38.950225Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Loading base model (FP16)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7adcc5dc5fee4aa6bfe52d980109b611"}},"metadata":{}},{"name":"stdout","text":"Loading adapter...\nMerging...\nSaving as FP16...\nUploading to HuggingFace...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0052064755a747d9b77c719aedc17ed4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2c558464d984d63ac5e40852dc1d55f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14cfdab152444b558a0f9a894c2a9c1c"}},"metadata":{}},{"name":"stdout","text":"âœ… Done!\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"from safetensors.torch import load_file, save_file\nimport re\nadapter_path = \"/kaggle/working/deepseek-leetcode/checkpoint-891\"\n# Load original weights\nweights = load_file(f\"{adapter_path}/adapter_model.safetensors\")\n# Fix the nested prefix\nfixed_weights = {}\nfor key, value in weights.items():\n    # Remove all the extra base_model.model. prefixes, keep just one\n    new_key = re.sub(r'^(base_model\\.model\\.)+', 'base_model.model.', key)\n    print(f\"Renaming: {key[:50]}... -> {new_key[:50]}...\")\n    fixed_weights[new_key] = value\n# Save fixed weights\nsave_file(fixed_weights, f\"{adapter_path}/adapter_model_fixed.safetensors\")\nprint(\"âœ… Fixed adapter saved!\")\n# Now rename to use the fixed one\nimport shutil\nshutil.move(f\"{adapter_path}/adapter_model.safetensors\", f\"{adapter_path}/adapter_model_backup.safetensors\")\nshutil.move(f\"{adapter_path}/adapter_model_fixed.safetensors\", f\"{adapter_path}/adapter_model.safetensors\")\nprint(\"âœ… Replaced original with fixed version!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T01:23:38.981202Z","iopub.execute_input":"2025-12-22T01:23:38.981508Z","iopub.status.idle":"2025-12-22T01:23:39.165670Z","shell.execute_reply.started":"2025-12-22T01:23:38.981481Z","shell.execute_reply":"2025-12-22T01:23:39.164985Z"}},"outputs":[{"name":"stdout","text":"Renaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.0.mlp.down_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.0.mlp.down_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.0.mlp.gate_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.0.mlp.gate_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.0.mlp.up_proj.lora_A...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.0.mlp.up_proj.lora_B...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.0.self_attn.k_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.0.self_attn.k_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.0.self_attn.o_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.0.self_attn.o_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.0.self_attn.q_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.0.self_attn.q_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.0.self_attn.v_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.0.self_attn.v_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.1.mlp.down_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.1.mlp.down_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.1.mlp.gate_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.1.mlp.gate_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.1.mlp.up_proj.lora_A...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.1.mlp.up_proj.lora_B...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.1.self_attn.k_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.1.self_attn.k_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.1.self_attn.o_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.1.self_attn.o_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.1.self_attn.q_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.1.self_attn.q_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.1.self_attn.v_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.1.self_attn.v_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.10.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.10.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.10.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.10.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.10.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.10.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.10.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.10.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.10.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.10.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.10.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.10.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.10.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.10.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.11.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.11.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.11.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.11.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.11.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.11.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.11.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.11.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.11.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.11.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.11.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.11.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.11.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.11.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.12.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.12.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.12.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.12.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.12.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.12.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.12.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.12.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.12.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.12.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.12.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.12.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.12.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.12.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.13.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.13.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.13.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.13.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.13.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.13.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.13.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.13.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.13.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.13.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.13.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.13.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.13.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.13.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.14.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.14.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.14.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.14.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.14.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.14.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.14.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.14.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.14.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.14.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.14.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.14.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.14.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.14.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.15.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.15.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.15.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.15.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.15.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.15.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.15.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.15.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.15.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.15.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.15.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.15.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.15.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.15.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.16.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.16.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.16.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.16.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.16.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.16.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.16.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.16.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.16.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.16.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.16.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.16.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.16.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.16.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.17.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.17.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.17.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.17.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.17.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.17.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.17.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.17.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.17.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.17.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.17.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.17.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.17.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.17.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.18.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.18.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.18.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.18.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.18.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.18.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.18.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.18.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.18.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.18.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.18.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.18.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.18.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.18.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.19.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.19.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.19.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.19.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.19.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.19.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.19.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.19.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.19.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.19.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.19.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.19.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.19.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.19.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.2.mlp.down_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.2.mlp.down_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.2.mlp.gate_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.2.mlp.gate_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.2.mlp.up_proj.lora_A...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.2.mlp.up_proj.lora_B...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.2.self_attn.k_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.2.self_attn.k_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.2.self_attn.o_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.2.self_attn.o_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.2.self_attn.q_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.2.self_attn.q_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.2.self_attn.v_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.2.self_attn.v_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.20.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.20.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.20.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.20.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.20.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.20.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.20.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.20.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.20.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.20.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.20.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.20.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.20.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.20.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.21.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.21.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.21.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.21.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.21.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.21.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.21.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.21.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.21.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.21.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.21.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.21.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.21.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.21.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.22.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.22.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.22.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.22.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.22.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.22.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.22.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.22.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.22.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.22.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.22.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.22.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.22.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.22.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.23.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.23.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.23.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.23.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.23.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.23.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.23.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.23.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.23.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.23.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.23.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.23.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.23.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.23.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.24.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.24.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.24.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.24.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.24.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.24.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.24.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.24.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.24.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.24.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.24.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.24.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.24.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.24.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.25.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.25.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.25.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.25.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.25.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.25.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.25.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.25.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.25.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.25.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.25.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.25.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.25.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.25.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.26.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.26.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.26.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.26.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.26.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.26.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.26.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.26.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.26.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.26.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.26.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.26.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.26.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.26.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.27.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.27.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.27.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.27.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.27.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.27.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.27.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.27.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.27.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.27.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.27.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.27.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.27.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.27.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.28.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.28.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.28.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.28.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.28.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.28.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.28.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.28.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.28.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.28.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.28.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.28.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.28.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.28.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.29.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.29.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.29.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.29.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.29.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.29.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.29.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.29.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.29.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.29.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.29.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.29.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.29.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.29.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.3.mlp.down_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.3.mlp.down_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.3.mlp.gate_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.3.mlp.gate_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.3.mlp.up_proj.lora_A...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.3.mlp.up_proj.lora_B...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.3.self_attn.k_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.3.self_attn.k_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.3.self_attn.o_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.3.self_attn.o_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.3.self_attn.q_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.3.self_attn.q_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.3.self_attn.v_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.3.self_attn.v_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.30.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.30.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.30.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.30.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.30.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.30.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.30.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.30.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.30.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.30.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.30.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.30.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.30.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.30.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.31.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.31.mlp.down_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.31.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.31.mlp.gate_proj.lor...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.31.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.31.mlp.up_proj.lora_...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.31.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.31.self_attn.k_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.31.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.31.self_attn.o_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.31.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.31.self_attn.q_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.31.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.31.self_attn.v_proj....\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.4.mlp.down_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.4.mlp.down_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.4.mlp.gate_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.4.mlp.gate_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.4.mlp.up_proj.lora_A...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.4.mlp.up_proj.lora_B...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.4.self_attn.k_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.4.self_attn.k_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.4.self_attn.o_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.4.self_attn.o_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.4.self_attn.q_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.4.self_attn.q_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.4.self_attn.v_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.4.self_attn.v_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.5.mlp.down_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.5.mlp.down_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.5.mlp.gate_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.5.mlp.gate_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.5.mlp.up_proj.lora_A...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.5.mlp.up_proj.lora_B...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.5.self_attn.k_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.5.self_attn.k_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.5.self_attn.o_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.5.self_attn.o_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.5.self_attn.q_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.5.self_attn.q_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.5.self_attn.v_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.5.self_attn.v_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.6.mlp.down_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.6.mlp.down_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.6.mlp.gate_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.6.mlp.gate_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.6.mlp.up_proj.lora_A...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.6.mlp.up_proj.lora_B...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.6.self_attn.k_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.6.self_attn.k_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.6.self_attn.o_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.6.self_attn.o_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.6.self_attn.q_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.6.self_attn.q_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.6.self_attn.v_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.6.self_attn.v_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.7.mlp.down_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.7.mlp.down_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.7.mlp.gate_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.7.mlp.gate_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.7.mlp.up_proj.lora_A...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.7.mlp.up_proj.lora_B...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.7.self_attn.k_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.7.self_attn.k_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.7.self_attn.o_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.7.self_attn.o_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.7.self_attn.q_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.7.self_attn.q_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.7.self_attn.v_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.7.self_attn.v_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.8.mlp.down_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.8.mlp.down_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.8.mlp.gate_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.8.mlp.gate_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.8.mlp.up_proj.lora_A...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.8.mlp.up_proj.lora_B...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.8.self_attn.k_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.8.self_attn.k_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.8.self_attn.o_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.8.self_attn.o_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.8.self_attn.q_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.8.self_attn.q_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.8.self_attn.v_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.8.self_attn.v_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.9.mlp.down_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.9.mlp.down_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.9.mlp.gate_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.9.mlp.gate_proj.lora...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.9.mlp.up_proj.lora_A...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.9.mlp.up_proj.lora_B...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.9.self_attn.k_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.9.self_attn.k_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.9.self_attn.o_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.9.self_attn.o_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.9.self_attn.q_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.9.self_attn.q_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.9.self_attn.v_proj.l...\nRenaming: base_model.model.base_model.model.base_model.model... -> base_model.model.model.layers.9.self_attn.v_proj.l...\nâœ… Fixed adapter saved!\nâœ… Replaced original with fixed version!\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Save and Download\ntrainer.save_model(\"/kaggle/working/deepseek-leetcode-final\")\ntokenizer.save_pretrained(\"/kaggle/working/deepseek-leetcode-final\")\n\n!zip -r /kaggle/working/deepseek-leetcode-adapter.zip /kaggle/working/deepseek-leetcode-final\nprint(\"âœ… Download deepseek-leetcode-adapter.zip from Output tab\")\n!ls -la /kaggle/working/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q huggingface_hub\nfrom huggingface_hub import login\nlogin()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T01:14:54.453827Z","iopub.execute_input":"2025-12-22T01:14:54.454429Z","iopub.status.idle":"2025-12-22T01:15:01.473121Z","shell.execute_reply.started":"2025-12-22T01:14:54.454395Z","shell.execute_reply":"2025-12-22T01:15:01.472151Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9387b99c6b1748c6bda57ab303b05600"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# Push merged model to HF Hub\nmodel.push_to_hub(\"Jerry-lin23/deepseek-leetcode-p3\")\ntokenizer.push_to_hub(\"Jerry-lin23/deepseek-leetcode-p3\")\nprint(\"âœ… Uploaded to Hugging Face!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-22T01:32:07.598156Z","iopub.execute_input":"2025-12-22T01:32:07.598825Z","iopub.status.idle":"2025-12-22T01:33:02.896847Z","shell.execute_reply.started":"2025-12-22T01:32:07.598798Z","shell.execute_reply":"2025-12-22T01:33:02.896217Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2358680c4324feb8729f237869dc23a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2953096a34a24dfd8b6b45c31db1c74d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88a78a577a0048089c40924fa94a3dd0"}},"metadata":{}},{"name":"stdout","text":"âœ… Uploaded to Hugging Face!\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# After trainer.train() completes, MERGE the model before saving\nprint(\"Merging LoRA into base model...\")\nmerged_model = model.merge_and_unload()\n# Save the MERGED model (not just adapter)\nmerged_model.save_pretrained(\"/kaggle/working/merged_model\")\ntokenizer.save_pretrained(\"/kaggle/working/merged_model\")\n# Zip for download\n!zip -r /kaggle/working/merged_model.zip /kaggle/working/merged_model\nprint(\"âœ… Download merged_model.zip from Output tab\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fresh Kaggle session - HF to GGUF\n!pip install -q transformers huggingface_hub\n!git clone https://github.com/ggerganov/llama.cpp\n!pip install -q -r llama.cpp/requirements.txt\nfrom huggingface_hub import snapshot_download\nsnapshot_download(\n    \"Jerry-lin23/deepseek-leetcode-merged\",\n    local_dir=\"/kaggle/working/merged\",\n    local_dir_use_symlinks=False\n)\nprint(\"âœ… Downloaded!\")\n!python llama.cpp/convert_hf_to_gguf.py /kaggle/working/merged \\\n    --outfile /kaggle/working/deepseek-leetcode-q8.gguf \\\n    --outtype q8_0\nprint(\"âœ… Converted!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T20:10:15.691904Z","iopub.execute_input":"2025-12-20T20:10:15.692194Z","iopub.status.idle":"2025-12-20T20:15:32.154058Z","shell.execute_reply.started":"2025-12-20T20:10:15.692163Z","shell.execute_reply":"2025-12-20T20:15:32.153023Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'llama.cpp'...\nremote: Enumerating objects: 72866, done.\u001b[K\nremote: Counting objects: 100% (269/269), done.\u001b[K\nremote: Compressing objects: 100% (194/194), done.\u001b[K\nremote: Total 72866 (delta 173), reused 75 (delta 75), pack-reused 72597 (from 2)\u001b[K\nReceiving objects: 100% (72866/72866), 249.19 MiB | 40.72 MiB/s, done.\nResolving deltas: 100% (52648/52648), done.\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m178.6/178.6 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m118.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m343.6/343.6 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngrain 0.2.15 requires protobuf>=5.28.3, but you have protobuf 4.25.8 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\njaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\njax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\npytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\ntorchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.6.0+cpu which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\ntorchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.6.0+cpu which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54960fdcb5a34d2abe70d58aef14b8fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f891811541b246f0b2233c32eae701b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2667dd1cfcb84421b636c71dcad6a0d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/3.59G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5b4f5fc87264fd6a85c43f3775e0ae5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eceb0c3cd7e4afb83d1c8534da22ac8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e19403987607430eb68e77bd313c5520"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/747 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a427a5680f74a17ae3d9f28aa9e84ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea66df0df5fc447bafa715d190a8280e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adf39b9df9de4f40b9dc19de766f718d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01b4eee5efff4fd9ad720d782ade7f5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e4a23be280145509bee84ad15edc73f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e5fd489923f470da98fe6ae00140ae3"}},"metadata":{}},{"name":"stdout","text":"âœ… Downloaded!\nINFO:hf-to-gguf:Loading model: merged\nINFO:hf-to-gguf:Model architecture: LlamaForCausalLM\nINFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\nINFO:hf-to-gguf:gguf: indexing model part 'model-00001-of-00003.safetensors'\nINFO:hf-to-gguf:gguf: indexing model part 'model-00002-of-00003.safetensors'\nINFO:hf-to-gguf:gguf: indexing model part 'model-00003-of-00003.safetensors'\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Exporting model...\nINFO:hf-to-gguf:token_embd.weight,           torch.float16 --> Q8_0, shape = {4096, 32256}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:output.weight,               torch.float16 --> Q8_0, shape = {4096, 32256}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.30.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.30.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.30.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.30.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.float16 --> Q8_0, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.31.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.31.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.31.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.31.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\nINFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:Set meta model\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 16384\nINFO:hf-to-gguf:gguf: embedding length = 4096\nINFO:hf-to-gguf:gguf: feed forward length = 11008\nINFO:hf-to-gguf:gguf: head count = 32\nINFO:hf-to-gguf:gguf: key-value head count = 32\nINFO:hf-to-gguf:gguf: rope scaling type = LINEAR\nINFO:hf-to-gguf:gguf: rope theta = 100000\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\nINFO:hf-to-gguf:gguf: file type = 7\nINFO:hf-to-gguf:Set model quantization version\nINFO:hf-to-gguf:Set model tokenizer\nINFO:numexpr.utils:NumExpr defaulting to 4 threads.\nWARNING:gguf.vocab:Unknown separator token '<ï½œbeginâ–ofâ–sentenceï½œ>' in TemplateProcessing<pair>\nINFO:gguf.vocab:Adding 31757 merge(s).\nINFO:gguf.vocab:Setting special token type bos to 32013\nINFO:gguf.vocab:Setting special token type eos to 32014\nINFO:gguf.vocab:Setting special token type pad to 32014\nINFO:gguf.vocab:Setting add_bos_token to True\nINFO:gguf.vocab:Setting add_sep_token to False\nINFO:gguf.vocab:Setting add_eos_token to False\nINFO:gguf.gguf_writer:Writing the following files:\nINFO:gguf.gguf_writer:/kaggle/working/deepseek-leetcode-q8.gguf: n_tensors = 291, total_size = 7.2G\nWriting:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 7.04G/7.16G [01:56<00:01, 60.0Mbyte/s]Traceback (most recent call last):\n  File \"/kaggle/working/llama.cpp/convert_hf_to_gguf.py\", line 10791, in <module>\n    main()\n  File \"/kaggle/working/llama.cpp/convert_hf_to_gguf.py\", line 10785, in main\n    model_instance.write()\n  File \"/kaggle/working/llama.cpp/convert_hf_to_gguf.py\", line 676, in write\n    self.gguf_writer.write_tensors_to_file(progress=True)\n  File \"/kaggle/working/llama.cpp/gguf-py/gguf/gguf_writer.py\", line 467, in write_tensors_to_file\n    ti.tensor.tofile(fout)\n  File \"/kaggle/working/llama.cpp/gguf-py/gguf/lazy.py\", line 226, in tofile\n    return eager.tofile(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nOSError: Not enough free space to write 47906816 bytes\nWriting:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 7.04G/7.16G [01:57<00:01, 59.8Mbyte/s]\nâœ… Converted!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Step 1: Re-download\nfrom huggingface_hub import snapshot_download\nprint(\"Downloading...\")\nsnapshot_download(\"Jerry-lin23/deepseek-leetcode-merged\", local_dir=\"/kaggle/working/merged\")\n# Step 2: Convert to F16\nprint(\"Converting to F16...\")\n!python llama.cpp/convert_hf_to_gguf.py /kaggle/working/merged \\\n    --outfile /kaggle/working/temp-f16.gguf \\\n    --outtype f16\n# Step 3: Delete merged to free space for quantization\n!rm -rf /kaggle/working/merged\n# Step 4: Build quantizer and quantize to Q4\nprint(\"Building quantizer...\")\n!cd llama.cpp && make -j4 llama-quantize\nprint(\"Quantizing to Q4...\")\n!./llama.cpp/llama-quantize /kaggle/working/temp-f16.gguf \\\n    /kaggle/working/deepseek-leetcode-q4.gguf Q4_K_M","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T20:48:40.241391Z","iopub.execute_input":"2025-12-20T20:48:40.241751Z","iopub.status.idle":"2025-12-20T20:50:13.455131Z","shell.execute_reply.started":"2025-12-20T20:48:40.241718Z","shell.execute_reply":"2025-12-20T20:50:13.454345Z"}},"outputs":[{"name":"stdout","text":"Downloading...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75aed4600a6043d28228552beecb2c69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"943a6fafa7014948b76a205e4b2e5383"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/3.59G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c2315ca6fd34ba3a52c839204fe0bc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/747 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65a5117aba93475aa40c714b10ae1bab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58af0991815148448c2c63e2e3b80c37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9142951e4617477690ea068664ca70e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdfd8c23261a4b2aa8e49b83251527f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"463791d129ee4955b9fdb2e5b86470d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8fa4c83e3434228ac9ae34ef06ed2a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e6f6b3c9a3f41c5b8bea3047378184d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c37a7ea1e5d1439687b53099a92662ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dad28520a3de4879a93f7cd65bfec92b"}},"metadata":{}},{"name":"stdout","text":"Converting to F16...\nINFO:hf-to-gguf:Loading model: merged\nINFO:hf-to-gguf:Model architecture: LlamaForCausalLM\nINFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\nINFO:hf-to-gguf:gguf: indexing model part 'model-00001-of-00003.safetensors'\nINFO:hf-to-gguf:gguf: indexing model part 'model-00002-of-00003.safetensors'\nINFO:hf-to-gguf:gguf: indexing model part 'model-00003-of-00003.safetensors'\nINFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\nINFO:hf-to-gguf:Exporting model...\nINFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {4096, 32256}\nINFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:output.weight,               torch.float16 --> F16, shape = {4096, 32256}\nINFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.30.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.30.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.30.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.30.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.float16 --> F16, shape = {11008, 4096}\nINFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 11008}\nINFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:blk.31.attn_k.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.31.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.31.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:blk.31.attn_v.weight,        torch.float16 --> F16, shape = {4096, 4096}\nINFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {4096}\nINFO:hf-to-gguf:Set meta model\nINFO:hf-to-gguf:Set model parameters\nINFO:hf-to-gguf:gguf: context length = 16384\nINFO:hf-to-gguf:gguf: embedding length = 4096\nINFO:hf-to-gguf:gguf: feed forward length = 11008\nINFO:hf-to-gguf:gguf: head count = 32\nINFO:hf-to-gguf:gguf: key-value head count = 32\nINFO:hf-to-gguf:gguf: rope scaling type = LINEAR\nINFO:hf-to-gguf:gguf: rope theta = 100000\nINFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\nINFO:hf-to-gguf:gguf: file type = 1\nINFO:hf-to-gguf:Set model quantization version\nINFO:hf-to-gguf:Set model tokenizer\nINFO:numexpr.utils:NumExpr defaulting to 4 threads.\nWARNING:gguf.vocab:Unknown separator token '<ï½œbeginâ–ofâ–sentenceï½œ>' in TemplateProcessing<pair>\nINFO:gguf.vocab:Adding 31757 merge(s).\nINFO:gguf.vocab:Setting special token type bos to 32013\nINFO:gguf.vocab:Setting special token type eos to 32014\nINFO:gguf.vocab:Setting special token type pad to 32014\nINFO:gguf.vocab:Setting add_bos_token to True\nINFO:gguf.vocab:Setting add_sep_token to False\nINFO:gguf.vocab:Setting add_eos_token to False\nINFO:gguf.gguf_writer:Writing the following files:\nINFO:gguf.gguf_writer:/kaggle/working/temp-f16.gguf: n_tensors = 291, total_size = 13.5G\nWriting:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 7.04G/13.5G [00:40<00:37, 172Mbyte/s]Traceback (most recent call last):\n  File \"/kaggle/working/llama.cpp/convert_hf_to_gguf.py\", line 10791, in <module>\n    main()\n  File \"/kaggle/working/llama.cpp/convert_hf_to_gguf.py\", line 10785, in main\n    model_instance.write()\n  File \"/kaggle/working/llama.cpp/convert_hf_to_gguf.py\", line 676, in write\n    self.gguf_writer.write_tensors_to_file(progress=True)\n  File \"/kaggle/working/llama.cpp/gguf-py/gguf/gguf_writer.py\", line 467, in write_tensors_to_file\n    ti.tensor.tofile(fout)\n  File \"/kaggle/working/llama.cpp/gguf-py/gguf/lazy.py\", line 226, in tofile\n    return eager.tofile(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nOSError: Not enough free space to write 33554432 bytes\nWriting:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 7.04G/13.5G [00:41<00:37, 170Mbyte/s]\nBuilding quantizer...\nMakefile:6: *** Build system changed:\n The Makefile build has been replaced by CMake.\n\n For build instructions see:\n https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md\n\n.  Stop.\nQuantizing to Q4...\n/bin/bash: line 1: ./llama.cpp/llama-quantize: No such file or directory\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Cell 1: Setup\n!git clone https://github.com/ggerganov/llama.cpp\n!pip install -q -r llama.cpp/requirements.txt\n# Cell 2: Find the model path\n!echo \"Looking for config.json...\"\n!find /kaggle/input/jerry -name \"config.json\"\n!echo \"\"\n!echo \"Full folder structure:\"\n!find /kaggle/input/jerry -type d","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T20:38:34.424385Z","iopub.execute_input":"2025-12-20T20:38:34.424698Z","iopub.status.idle":"2025-12-20T20:38:38.818663Z","shell.execute_reply.started":"2025-12-20T20:38:34.424665Z","shell.execute_reply":"2025-12-20T20:38:38.817976Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'llama.cpp' already exists and is not an empty directory.\nLooking for config.json...\n\nFull folder structure:\n/kaggle/input/jerry\n/kaggle/input/jerry/pytorch\n/kaggle/input/jerry/pytorch/default\n/kaggle/input/jerry/pytorch/default/1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!ls -la /kaggle/input/jerry/pytorch/default/1/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T20:39:35.422833Z","iopub.execute_input":"2025-12-20T20:39:35.423577Z","iopub.status.idle":"2025-12-20T20:39:35.542909Z","shell.execute_reply.started":"2025-12-20T20:39:35.423546Z","shell.execute_reply":"2025-12-20T20:39:35.542111Z"}},"outputs":[{"name":"stdout","text":"total 144\ndrwxr-xr-x 2 nobody nogroup      0 Dec 20 20:34 .\ndrwxr-xr-x 3 root   root      4096 Dec 20 20:34 ..\n-rw-r--r-- 1 nobody nogroup 141551 Dec 20 20:34 deepseek-leetcode-merged\n","output_type":"stream"}],"execution_count":3}]}